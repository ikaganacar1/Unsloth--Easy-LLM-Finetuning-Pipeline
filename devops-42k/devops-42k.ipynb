{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d5ae12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42819/42819 [00:00<00:00, 487351.64 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "df = load_dataset(\"Mubeen161/DEVOPS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ad15da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data_dict = df['train'].to_dict()\n",
    "with open('devops-dataset.json', 'w') as f:\n",
    "        json.dump(data_dict, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89ba416b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Permanent browser cache using ServiceWorker\n",
      "\n",
      "\n",
      "I am designing a JavaScript secure loader. The loader is inlined in the index.html. The goal of the secure loader is to only load JavaScript resources are trusted. The contents of index.html are mostly limited to the secure loader. For security purposes, I want index.html (as stored in cache) to never change, even if my website is hacked.\n",
      "How can I cache index.html without the server being able to tamper with the cache? I am wondering if ServiceWorkers can help. Effectively, the index.html would register a service worker for fetching itself from an immutable cache (no network request is even made).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            4\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                +100\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "in chrome you can use FileSystem API\n",
      "http://www.noupe.com/design/html5-filesystem-api-create-files-store-locally-using-javascript-webkit.html this allows you to then save and read files from a sand-boxed file-system though the browser.\n",
      "As for other support it's not been confirmed as an addition to the HTML5 specification set yet. so it's only available in chrome.\n",
      "You could also use the IndexDB system this is supported in all modern browsers.\n",
      "you can use both these services inside a Service Worker to manage the loading and manage of the content however i have to question why would you want to you prevent your self from ever updating your index.html\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Share\n",
      "\n",
      "\n",
      "Improve this answer\n",
      "\n",
      "\n",
      "\n",
      "                        Follow\n",
      "                    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "edited Feb 15, 2017 at 20:48\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            answered Feb 9, 2017 at 0:56\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Barkermn01Barkermn01\n",
      "\n",
      "6,8193535 silver badges8484 bronze badges\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note that the Filesystem API (formal title: File API: Directories and System) is obsolete and no other browsers have any plans to ever implement support for it. The specification itself has been demoted to a W3C ‚ÄúNote‚Äù with a warning in bold: Work on this document has been discontinued and it should not be referenced or used as a basis for implementation. So while it‚Äôs fine to use for the case where somebody might only care about having something that works just in Chrome, they should not expect to see it working in any other browsers, ever.\n",
      "\n",
      "‚Äì¬†sideshowbarker\n",
      "‚ô¶ \n",
      "Feb 12, 2017 at 3:04\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "BTW this is why we don't trust W3C on HTML5 they standardised something that should not have been standardised updated on the 11th of January 2017, wicg.github.io/entries-api \" Other browsers (at this time: Microsoft Edge and Mozilla Firefox) are starting to support subsets of Chrome‚Äôs APIs and behavior.\"\n",
      "\n",
      "‚Äì¬†Barkermn01\n",
      "\n",
      "Feb 22, 2017 at 8:27\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Add a comment\n",
      "¬†|¬†\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('devops-42k.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "print(data.get(\"Prompt\")[0])\n",
    "print()\n",
    "print(data.get(\"Instruction\")[0])\n",
    "print()\n",
    "print(data.get(\"Response\")[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3c9049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from html import unescape\n",
    "\n",
    "class DatasetCleaner:\n",
    "    def __init__(self):\n",
    "        # Compile regex patterns for better performance\n",
    "        self.patterns = {\n",
    "            'html_tags': re.compile(r'<[^>]+>'),\n",
    "            'stackoverflow_artifacts': re.compile(r'\\b(Share|Improve this answer|Follow|edited|answered|Add a comment)\\b.*', re.IGNORECASE),\n",
    "            'user_reputation': re.compile(r',\\s*\\d{1,6}\\s*(?:\\d+\\s*(?:silver|bronze|gold)\\s*badges?)*'),\n",
    "            'username_reputation': re.compile(r'\\b[A-Za-z0-9_]+\\b\\s*\\d{1,6}\\s*(?:\\d+\\s*(?:silver|bronze|gold)\\s*badges?)*'),\n",
    "            'isolated_numbers': re.compile(r'^\\s*\\d+\\s*$', re.MULTILINE),\n",
    "            'number_patterns': re.compile(r'\\s+\\d{1,4}\\s+'),\n",
    "            'timestamps': re.compile(r'\\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{1,2},?\\s+\\d{4}(?:\\s+at\\s+\\d{1,2}:\\d{2})?'),\n",
    "            'time_colons': re.compile(r',\\s*\\d{4}\\s*:\\d{2}'),\n",
    "            'user_mentions': re.compile(r'‚Äì\\s*[A-Za-z0-9_]+'),\n",
    "            'retryable_errors': re.compile(r'\\[!\\]\\s*retryable error.*', re.DOTALL),\n",
    "            'duplicate_questions': re.compile(r'This question already has an answer here:.*', re.DOTALL),\n",
    "            'answer_counts': re.compile(r'\\(\\d+\\s*answer\\)'),\n",
    "            'time_ago': re.compile(r'\\b\\d+\\s*(?:year|month|day|hour|minute)s?\\s+ago\\.?'),\n",
    "            'duplicate_markers': re.compile(r'\\[duplicate\\]'),\n",
    "            'common_artifacts': re.compile(r'\\b(HIH|Hope it helps?)\\b'),\n",
    "            'revision_thanks': re.compile(r'Revision:\\s*\\(thanks.*?\\)'),\n",
    "            'multiple_newlines': re.compile(r'\\n\\s*\\n\\s*\\n+'),\n",
    "            'multiple_spaces': re.compile(r'[ \\t]+'),\n",
    "            'meaningless_lines': re.compile(r'^[\\d\\s\\+\\-\\|\\*\\.,;:]+$'),\n",
    "            'no_alphanumeric': re.compile(r'^[^\\w]*$'),\n",
    "            'meaningful_words': re.compile(r'\\b[a-zA-Z]{3,}\\b')\n",
    "        }\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"\n",
    "        Clean text by removing HTML tags, extra whitespace, and other artifacts\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to string if not already\n",
    "        text = str(text)\n",
    "        \n",
    "        # Unescape HTML entities\n",
    "        text = unescape(text)\n",
    "        \n",
    "        # Remove HTML tags\n",
    "        text = self.patterns['html_tags'].sub('', text)\n",
    "        \n",
    "        # Remove Stack Overflow specific artifacts\n",
    "        text = self.patterns['stackoverflow_artifacts'].sub('', text)\n",
    "        \n",
    "        # Remove user reputation patterns\n",
    "        text = self.patterns['user_reputation'].sub('', text)\n",
    "        text = self.patterns['username_reputation'].sub('', text)\n",
    "        \n",
    "        # Remove isolated numbers and vote counts\n",
    "        text = self.patterns['isolated_numbers'].sub('', text)\n",
    "        text = self.patterns['number_patterns'].sub(' ', text)\n",
    "        \n",
    "        # Remove timestamps and dates\n",
    "        text = self.patterns['timestamps'].sub('', text)\n",
    "        text = self.patterns['time_colons'].sub('', text)\n",
    "        text = self.patterns['user_mentions'].sub('', text)\n",
    "        \n",
    "        # Remove specific Stack Overflow artifacts\n",
    "        text = self.patterns['retryable_errors'].sub('', text)\n",
    "        text = self.patterns['duplicate_questions'].sub('', text)\n",
    "        text = self.patterns['answer_counts'].sub('', text)\n",
    "        text = self.patterns['time_ago'].sub('', text)\n",
    "        text = self.patterns['duplicate_markers'].sub('', text)\n",
    "        \n",
    "        # Remove common code artifacts and noise\n",
    "        text = self.patterns['common_artifacts'].sub('', text)\n",
    "        text = self.patterns['revision_thanks'].sub('', text)\n",
    "        \n",
    "        # Clean up extra whitespace\n",
    "        text = self.patterns['multiple_newlines'].sub('\\n\\n', text)\n",
    "        text = self.patterns['multiple_spaces'].sub(' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        # Remove lines that are just numbers, symbols, or very short\n",
    "        lines = text.split('\\n')\n",
    "        cleaned_lines = []\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            # Skip if line is empty, just numbers/symbols, or too short\n",
    "            if (line and \n",
    "                len(line) > 3 and\n",
    "                not self.patterns['meaningless_lines'].match(line) and\n",
    "                not self.patterns['no_alphanumeric'].match(line)):\n",
    "                cleaned_lines.append(line)\n",
    "        \n",
    "        return '\\n'.join(cleaned_lines)\n",
    "    \n",
    "    def is_valid_content(self, text):\n",
    "        \"\"\"\n",
    "        Check if the content is valid (not too short, contains meaningful text)\n",
    "        \"\"\"\n",
    "        if not text or len(text.strip()) < 15:\n",
    "            return False\n",
    "        \n",
    "        # Check if it's mostly just symbols or numbers\n",
    "        alphanumeric = re.sub(r'[^a-zA-Z0-9]', '', text)\n",
    "        if len(alphanumeric) < 10:\n",
    "            return False\n",
    "        \n",
    "        # Check for meaningful words (at least 3 words with 3+ characters)\n",
    "        words = self.patterns['meaningful_words'].findall(text)\n",
    "        if len(words) < 3:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def clean_and_convert_dataset(self, input_file, output_file):\n",
    "        \"\"\"\n",
    "        Clean the messy dataset and convert to ChatML format\n",
    "        \"\"\"\n",
    "        print(f\"Loading dataset from {input_file}...\")\n",
    "        \n",
    "        try:\n",
    "            with open(input_file, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading dataset: {e}\")\n",
    "            return 0\n",
    "        \n",
    "        prompts = data.get(\"Prompt\", [])\n",
    "        instructions = data.get(\"Instruction\", [])\n",
    "        responses = data.get(\"Response\", [])\n",
    "        \n",
    "        print(f\"Found {len(prompts)} prompts, {len(instructions)} instructions, {len(responses)} responses\")\n",
    "        \n",
    "        cleaned_conversations = []\n",
    "        skipped = 0\n",
    "        \n",
    "        # Process each entry\n",
    "        total_entries = max(len(prompts), len(instructions), len(responses))\n",
    "        \n",
    "        for i in range(total_entries):\n",
    "            try:\n",
    "                # Get raw content\n",
    "                prompt = prompts[i] if i < len(prompts) else \"\"\n",
    "                instruction = instructions[i] if i < len(instructions) else \"\"\n",
    "                response = responses[i] if i < len(responses) else \"\"\n",
    "                \n",
    "                # Clean the content\n",
    "                clean_prompt = self.clean_text(prompt)\n",
    "                clean_instruction = self.clean_text(instruction)\n",
    "                clean_response = self.clean_text(response)\n",
    "                \n",
    "                # Combine prompt and instruction for user message\n",
    "                user_content = \"\"\n",
    "                if clean_prompt and clean_instruction:\n",
    "                    # If both exist, use prompt as title and instruction as details\n",
    "                    user_content = f\"{clean_prompt}\\n\\n{clean_instruction}\"\n",
    "                elif clean_prompt:\n",
    "                    user_content = clean_prompt\n",
    "                elif clean_instruction:\n",
    "                    user_content = clean_instruction\n",
    "                \n",
    "                # Validate content\n",
    "                if not self.is_valid_content(user_content) or not self.is_valid_content(clean_response):\n",
    "                    skipped += 1\n",
    "                    if i % 1000 == 0 and i > 0:\n",
    "                        print(f\"Processed {i}/{total_entries} entries, skipped {skipped} so far...\")\n",
    "                    continue\n",
    "                \n",
    "                # Create ChatML conversation\n",
    "                conversation = {\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": user_content.strip()\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"assistant\",\n",
    "                            \"content\": clean_response.strip()\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "                \n",
    "                cleaned_conversations.append(conversation)\n",
    "                \n",
    "                # Progress update\n",
    "                if i % 1000 == 0 and i > 0:\n",
    "                    print(f\"Processed {i}/{total_entries} entries, {len(cleaned_conversations)} valid conversations...\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing entry {i}: {e}\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "        \n",
    "        # Save to JSONL format\n",
    "        print(f\"Saving {len(cleaned_conversations)} conversations to {output_file}...\")\n",
    "        \n",
    "        try:\n",
    "            with open(output_file, 'w', encoding='utf-8') as file:\n",
    "                for conversation in cleaned_conversations:\n",
    "                    file.write(json.dumps(conversation, ensure_ascii=False) + '\\n')\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving dataset: {e}\")\n",
    "            return 0\n",
    "        \n",
    "        print(f\"‚úÖ Successfully cleaned and converted {len(cleaned_conversations)} conversations\")\n",
    "        print(f\"‚ùå Skipped {skipped} invalid entries\")\n",
    "        print(f\"üìÅ Output saved to: {output_file}\")\n",
    "        \n",
    "        return len(cleaned_conversations)\n",
    "    \n",
    "    def preview_cleaned_data(self, output_file, num_examples=3):\n",
    "        \"\"\"\n",
    "        Preview the cleaned data\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"PREVIEW OF CLEANED DATA\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            with open(output_file, 'r', encoding='utf-8') as file:\n",
    "                for i, line in enumerate(file):\n",
    "                    if i >= num_examples:\n",
    "                        break\n",
    "                    \n",
    "                    conversation = json.loads(line)\n",
    "                    print(f\"\\nüìù CONVERSATION {i+1}:\")\n",
    "                    print(f\"{'‚îÄ'*40}\")\n",
    "                    \n",
    "                    user_msg = conversation[\"messages\"][0][\"content\"]\n",
    "                    assistant_msg = conversation[\"messages\"][1][\"content\"]\n",
    "                    \n",
    "                    print(f\"üë§ USER ({len(user_msg)} chars):\")\n",
    "                    print(user_msg[:300] + \"...\" if len(user_msg) > 300 else user_msg)\n",
    "                    \n",
    "                    print(f\"\\nü§ñ ASSISTANT ({len(assistant_msg)} chars):\")\n",
    "                    print(assistant_msg[:300] + \"...\" if len(assistant_msg) > 300 else assistant_msg)\n",
    "                    \n",
    "                    print(f\"{'‚îÄ'*40}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error previewing data: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cae164c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ DevOps Dataset Cleaner and ChatML Converter\n",
      "==================================================\n",
      "Loading dataset from devops-42k.json...\n",
      "Found 42819 prompts, 42819 instructions, 42819 responses\n",
      "Processed 1000/42819 entries, 998 valid conversations...\n",
      "Processed 2000/42819 entries, 1998 valid conversations...\n",
      "Processed 3000/42819 entries, 2997 valid conversations...\n",
      "Processed 4000/42819 entries, 3995 valid conversations...\n",
      "Processed 5000/42819 entries, 4993 valid conversations...\n",
      "Processed 6000/42819 entries, 5989 valid conversations...\n",
      "Processed 7000/42819 entries, 6988 valid conversations...\n",
      "Processed 8000/42819 entries, 7987 valid conversations...\n",
      "Processed 9000/42819 entries, 8985 valid conversations...\n",
      "Processed 10000/42819 entries, 9983 valid conversations...\n",
      "Processed 11000/42819 entries, 10981 valid conversations...\n",
      "Processed 12000/42819 entries, 11981 valid conversations...\n",
      "Processed 13000/42819 entries, 12981 valid conversations...\n",
      "Processed 14000/42819 entries, 13979 valid conversations...\n",
      "Processed 15000/42819 entries, 14975 valid conversations...\n",
      "Processed 16000/42819 entries, 15973 valid conversations...\n",
      "Processed 17000/42819 entries, 16972 valid conversations...\n",
      "Processed 18000/42819 entries, 17970 valid conversations...\n",
      "Processed 19000/42819 entries, 18967 valid conversations...\n",
      "Processed 20000/42819 entries, 19965 valid conversations...\n",
      "Processed 21000/42819 entries, 20963 valid conversations...\n",
      "Processed 22000/42819 entries, 21958 valid conversations...\n",
      "Processed 23000/42819 entries, 22955 valid conversations...\n",
      "Processed 24000/42819 entries, 23952 valid conversations...\n",
      "Processed 25000/42819 entries, 24949 valid conversations...\n",
      "Processed 26000/42819 entries, 25947 valid conversations...\n",
      "Processed 27000/42819 entries, 26944 valid conversations...\n",
      "Processed 28000/42819 entries, 27942 valid conversations...\n",
      "Processed 29000/42819 entries, 28941 valid conversations...\n",
      "Processed 30000/42819 entries, 29940 valid conversations...\n",
      "Processed 31000/42819 entries, 30940 valid conversations...\n",
      "Processed 32000/42819 entries, 31938 valid conversations...\n",
      "Processed 33000/42819 entries, 32937 valid conversations...\n",
      "Processed 34000/42819 entries, 33935 valid conversations...\n",
      "Processed 35000/42819 entries, 34933 valid conversations...\n",
      "Processed 36000/42819 entries, 35930 valid conversations...\n",
      "Processed 37000/42819 entries, 36929 valid conversations...\n",
      "Processed 38000/42819 entries, 37927 valid conversations...\n",
      "Processed 39000/42819 entries, 38927 valid conversations...\n",
      "Processed 40000/42819 entries, 39927 valid conversations...\n",
      "Processed 41000/42819 entries, 40926 valid conversations...\n",
      "Processed 42000/42819 entries, 41926 valid conversations...\n",
      "Saving 42742 conversations to devops-42k-clean.jsonl...\n",
      "‚úÖ Successfully cleaned and converted 42742 conversations\n",
      "‚ùå Skipped 77 invalid entries\n",
      "üìÅ Output saved to: devops-42k-clean.jsonl\n",
      "\n",
      "============================================================\n",
      "PREVIEW OF CLEANED DATA\n",
      "============================================================\n",
      "\n",
      "üìù CONVERSATION 1:\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "üë§ USER (650 chars):\n",
      "Permanent browser cache using ServiceWorker\n",
      "\n",
      "I am designing a JavaScript secure loader. The loader is inlined in the index.html. The goal of the secure loader is to only load JavaScript resources are trusted. The contents of index.html are mostly limited to the secure loader. For security purposes, ...\n",
      "\n",
      "ü§ñ ASSISTANT (1508 chars):\n",
      "in chrome you can use FileSystem API\n",
      "http://www.noupe.com/design/html5-filesystem-api-create-files-store-locally-using-javascript-webkit.html this allows you to then save and read files from a sand-boxed file-system though the browser.\n",
      "As for other support it's not been confirmed as an addition to t...\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìù CONVERSATION 2:\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "üë§ USER (123 chars):\n",
      "GPU nodegroup in EKS\n",
      "\n",
      "I am not able to create a nodegroup with GPU type using EKS, getting this error from cloud formation:\n",
      "\n",
      "ü§ñ ASSISTANT (108 chars):\n",
      "the resolution was to install nividia plugins on the cluster so that the cluster will identify the gpu nodes\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚ú® Processing complete!\n",
      "üìä Statistics:\n",
      "   - Input file: devops-42k.json\n",
      "   - Output file: devops-42k-clean.jsonl\n",
      "   - Valid conversations: 42742\n",
      "   - Ready for training! üöÄ\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the dataset cleaning and conversion\n",
    "    \"\"\"\n",
    "    print(\"üßπ DevOps Dataset Cleaner and ChatML Converter\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    cleaner = DatasetCleaner()\n",
    "    \n",
    "    # Clean and convert the dataset\n",
    "    input_file = 'devops-42k.json'\n",
    "    output_file = 'devops-42k-clean.jsonl'\n",
    "    \n",
    "    num_conversations = cleaner.clean_and_convert_dataset(input_file, output_file)\n",
    "    \n",
    "    # Preview the results if successful\n",
    "    if num_conversations > 0:\n",
    "        cleaner.preview_cleaned_data(output_file, num_examples=2)\n",
    "        \n",
    "        print(f\"\\n‚ú® Processing complete!\")\n",
    "        print(f\"üìä Statistics:\")\n",
    "        print(f\"   - Input file: {input_file}\")\n",
    "        print(f\"   - Output file: {output_file}\")\n",
    "        print(f\"   - Valid conversations: {num_conversations}\")\n",
    "        print(f\"   - Ready for training! üöÄ\")\n",
    "    else:\n",
    "        print(\"‚ùå No valid conversations were generated. Check your input file.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
